# .github/workflows/scrape-upi-data.yml
# This GitHub Action runs automatically every day and scrapes fresh data

name: ğŸ¦ UPI Data Scraper

on:
  schedule:
    - cron: '0 6 * * *'  # Runs daily at 6 AM UTC (11:30 AM IST)
  workflow_dispatch:     # Allows manual trigger
  push:
    branches: [ main ]

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout Repository
      uses: actions/checkout@v3
      
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: ğŸ“¦ Install Dependencies
      run: |
        pip install requests beautifulsoup4 pandas lxml selenium
        # Install Chrome for Selenium
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo apt-get update
        sudo apt-get install google-chrome-stable
        
    - name: ğŸ•·ï¸ Run UPI Data Scraper
      run: python scraper.py
      
    - name: ğŸ“Š Update Dashboard Data
      run: python update_data.py
      
    - name: ğŸš€ Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs
        
    - name: ğŸ“§ Send Success Notification
      if: success()
      run: |
        echo "âœ… UPI Dashboard updated successfully!"
        echo "ğŸ“Š Data scraped at: $(date)"
        echo "ğŸŒ Live at: https://${{ github.repository_owner }}.github.io/upi-dashboard"
